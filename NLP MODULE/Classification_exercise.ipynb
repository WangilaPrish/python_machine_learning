{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Classification\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will review and assess our understanding of the core concepts of classifier model selection.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this exercise, you should be able to:\n",
    "* Build and evaluate multiple types of classification models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Analysing hate speech and offensive language in tweets\n",
    "\n",
    "Our dataset consists of roughly 5,600 tweets containing instances of hate speech and offensive language. These tweets have been curated to provide a focused dataset for building sentiment analysis and toxicity detection models. Each tweet reflects varying degrees of negativity, from casual derogatory remarks to explicit expressions of prejudice and intolerance.\n",
    "\n",
    "By examining this dataset, we aim to understand the prevalence and patterns of hate speech and offensive language in online discourse. Through data analysis, we seek insights into the factors driving such language, as well as its impact on digital communities. Ultimately, our goal is to develop tools and strategies for mitigating the spread of harmful language online and fostering a more inclusive and respectful online environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43039</th>\n",
       "      <td>i will beat a bitch ass tf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36956</th>\n",
       "      <td>thomasnye1 my momma saw how the girls danced a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>user user dont forget his other incantation i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27287</th>\n",
       "      <td>isnt it sad  how i keep thinking youll change ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56311</th>\n",
       "      <td>please tell this bitch im subbin her ik one of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429</th>\n",
       "      <td>animaladvocate  melodylgattenby zoo says this ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12737</th>\n",
       "      <td>alice doggy my petstagram instapets pet pets d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12503</th>\n",
       "      <td>h a p p y   w i n e   p a r t y  momentoafouna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53172</th>\n",
       "      <td>stupid teabagger restaurant making customers p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12529</th>\n",
       "      <td>user you guys live these rules loved working ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5674 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweet  Toxicity\n",
       "43039                         i will beat a bitch ass tf         1\n",
       "36956  thomasnye1 my momma saw how the girls danced a...         1\n",
       "8373    user user dont forget his other incantation i...         0\n",
       "27287  isnt it sad  how i keep thinking youll change ...         0\n",
       "56311  please tell this bitch im subbin her ik one of...         1\n",
       "...                                                  ...       ...\n",
       "6429   animaladvocate  melodylgattenby zoo says this ...         0\n",
       "12737  alice doggy my petstagram instapets pet pets d...         0\n",
       "12503  h a p p y   w i n e   p a r t y  momentoafouna...         0\n",
       "53172  stupid teabagger restaurant making customers p...         1\n",
       "12529   user you guys live these rules loved working ...         0\n",
       "\n",
       "[5674 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tweets_df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/toxicity_tweets_cleaned.csv', index_col=0)\n",
    "tweets_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are tasked with building multiple classifier models to predict whether a given tweet contains hate speech or offensive language. Our dataset consists of roughly 5,600 tweets, each accompanied by a label indicating whether it expresses toxicity.\n",
    "\n",
    "The objective is to develop robust machine learning models capable of accurately classifying tweets as toxic or non-toxic based on their content. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Before we can build our models, we need to first preprocess the text data. Preprocessing involves converting the text into a format that can be easily understood by the algorithms. Use `CountVectorizer` to transform the text data into a matrix where each row represents a tweet and each column represents a unique word in the vocabulary. \n",
    "\n",
    "Split the dataset into training and testing sets using a `80-20 split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4539\n",
      "Testing set size: 1135\n",
      "Number of features: 14747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the tweet text data and transform it\n",
    "X = vectorizer.fit_transform(tweets_df['Tweet'])\n",
    "\n",
    "# Convert the sparse matrix to an array\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Split the dataset into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_array,\n",
    "    tweets_df['Toxicity'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Now we can build classifier models using the training data and assess their performance on the testing data.\n",
    "\n",
    "Implement the following models: `Logistic Regression`, `Decision Tree`, `Support Vector Classification`, and `Nearest Neighbors`. Evaluate each model's performance using the following evaluation metrics: `accuracy`, `precision`, `recall`, and `F1 score`. Note: Running these models might take a few minutes, depending on the complexity chosen. \n",
    "\n",
    "In addition to this, calculate the confusion matrix for each of our models. \n",
    "\n",
    "What do these results tell us about our models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating classifiers...\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression completed.\n",
      "\n",
      "Training Decision Tree...\n",
      "Decision Tree completed.\n",
      "\n",
      "Training Support Vector Classification...\n",
      "Support Vector Classification completed.\n",
      "\n",
      "Training Nearest Neighbors...\n",
      "Nearest Neighbors completed.\n",
      "\n",
      "============================================================\n",
      "EVALUATION METRICS\n",
      "============================================================\n",
      "\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.9101\n",
      "  Precision: 0.9496\n",
      "  Recall: 0.8302\n",
      "  F1 Score: 0.8859\n",
      "\n",
      "Decision Tree:\n",
      "  Accuracy: 0.9154\n",
      "  Precision: 0.9224\n",
      "  Recall: 0.8721\n",
      "  F1 Score: 0.8966\n",
      "\n",
      "Support Vector Classification:\n",
      "  Accuracy: 0.8969\n",
      "  Precision: 0.9592\n",
      "  Recall: 0.7883\n",
      "  F1 Score: 0.8654\n",
      "\n",
      "Nearest Neighbors:\n",
      "  Accuracy: 0.7850\n",
      "  Precision: 0.7373\n",
      "  Recall: 0.7589\n",
      "  F1 Score: 0.7479\n",
      "\n",
      "============================================================\n",
      "CONFUSION MATRICES\n",
      "============================================================\n",
      "\n",
      "Logistic Regression:\n",
      "  [[TN=637, FP=21]\n",
      "   [FN=81, TP=396]]\n",
      "\n",
      "  Matrix:\n",
      "[[637  21]\n",
      " [ 81 396]]\n",
      "\n",
      "Decision Tree:\n",
      "  [[TN=623, FP=35]\n",
      "   [FN=61, TP=416]]\n",
      "\n",
      "  Matrix:\n",
      "[[623  35]\n",
      " [ 61 416]]\n",
      "\n",
      "Support Vector Classification:\n",
      "  [[TN=642, FP=16]\n",
      "   [FN=101, TP=376]]\n",
      "\n",
      "  Matrix:\n",
      "[[642  16]\n",
      " [101 376]]\n",
      "\n",
      "Nearest Neighbors:\n",
      "  [[TN=529, FP=129]\n",
      "   [FN=115, TP=362]]\n",
      "\n",
      "  Matrix:\n",
      "[[529 129]\n",
      " [115 362]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Initialize the classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Support Vector Classification\": SVC(random_state=42),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Dictionary to store results and confusion matrices\n",
    "results = {}\n",
    "conf_matrices = {}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "print(\"Training and evaluating classifiers...\\n\")\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "\n",
    "    # Calculate and store confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrices[name] = conf_matrix\n",
    "\n",
    "    print(f\"{name} completed.\\n\")\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Display confusion matrices\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\"*60)\n",
    "for name, conf_matrix in conf_matrices.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  [[TN={conf_matrix[0,0]}, FP={conf_matrix[0,1]}]\")\n",
    "    print(f\"   [FN={conf_matrix[1,0]}, TP={conf_matrix[1,1]}]]\")\n",
    "    print(f\"\\n  Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "In addition to the performance evaluation based on metrics and confusion matrices, cross-validation scores provide further insights into the robustness and generalisation capabilities of classifier models. \n",
    "\n",
    "After evaluating the performance of our classifier models, we want to determine the best model based on their cross-validation scores. \n",
    "\n",
    "Perform 5-fold cross-validation for each classifier model using the training data and print the `mean cross-validation score`.\n",
    "\n",
    "**Note**: This code should take a few minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Dictionary to store cross-validation scores\n",
    "cv_scores = {}\n",
    "\n",
    "print(\"Performing 5-fold cross-validation...\\n\")\n",
    "print(\"Note: This may take a few minutes to complete.\\n\")\n",
    "\n",
    "# Perform cross-validation for each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Running cross-validation for {name}...\")\n",
    "\n",
    "    # Perform 5-fold cross-validation and calculate mean score\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "\n",
    "    # Store the mean score\n",
    "    cv_scores[name] = mean_score\n",
    "\n",
    "    print(f\"  Fold scores: {scores}\")\n",
    "    print(f\"  Mean CV Score: {mean_score:.4f} (+/- {std_score:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Display summary of cross-validation scores\n",
    "print(\"=\"*60)\n",
    "print(\"CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for name, score in sorted(cv_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "# Identify the best model\n",
    "best_model = max(cv_scores, key=cv_scores.get)\n",
    "print(f\"\\nBest model based on cross-validation: {best_model} ({cv_scores[best_model]:.4f})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the tweet text data\n",
    "X = vectorizer.fit_transform(tweets_df['Tweet'])\n",
    "\n",
    "# Convert the sparse matrix to an array\n",
    "X_array = X.toarray()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training and testing sets\n",
    "# Split the data into features (X) and target labels (y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, tweets_df['Toxicity'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Initialize the classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Support Vector Classification\": SVC(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "conf_matrices = {}\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results[name] = {\"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1}\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrices[name] = conf_matrix\n",
    "\n",
    "# Display results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"Metrics for {name}:\")\n",
    "    print(metrics)\n",
    "    print()\n",
    "\n",
    "# Display confusion matrices\n",
    "for name, conf_matrix in conf_matrices.items():\n",
    "    print(f\"Confusion Matrix for {name}:\")\n",
    "    print(conf_matrix)\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using Python (3.10.14), our results seem to be best for the Decision Tree model in terms of its F1 score, boasting high accuracy, precision, recall, comparing favourably to the other classifiers. The Logistic Regression model follows closely, with its ability to correctly classify a significant proportion of samples, coupled with balanced precision and recall metrics, also showing its robustness in handling toxic and non-toxic instances. Support Vector Classification (SVC), while exhibiting high precision, falters in recall, leading to an imbalance between false negatives and false positives. Nearest Neighbors (KNN), with the lowest accuracy and F1 score, struggles to strike a balance between precision and recall, resulting in suboptimal predictive performance.\n",
    "\n",
    "It is however important to note a few things. Firstly, the skeleton for models provided here are only the start of the process of finding a suitable model. In reality, we cannot say with full certainty that the KNN model is less suitable than another if we've not attempted to find the optimal combination of hyperparameters (by not specifying the number of neighbours for instance, the default used here was 5). Secondly, if two models seem to perform similarly in terms of precision, accuracy and recall, it might be worth deciding whether False Positives are a **more wanted** phenomena than False Negatives. In the medical world, this might be prefereable. These findings underscore the importance of meticulously evaluating various classifiers and choosing the most suitable model based on specific task requirements and performance metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Dictionary to store cross-validation scores\n",
    "cv_scores = {}\n",
    "\n",
    "# Perform cross-validation for each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    # Perform 5-fold cross-validation and store the scores\n",
    "    cv_scores[name] = cross_val_score(clf, X_train, y_train, cv=5).mean()\n",
    "\n",
    "# Display cross-validation scores\n",
    "for name, scores in cv_scores.items():\n",
    "    print(f\"Cross-validation scores for {name}:\")\n",
    "    print(scores)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Logistic Regression` maintains its superiority with the highest cross-validation score of 0.904, affirming its consistency in performance across multiple data splits. `Decision Tree` follows closely, demonstrating stable performance with a cross-validation score of 0.895. However, `Support Vector Classification (SVC)` and `Nearest Neighbors` continue to lag behind, with scores of 0.889 and 0.785, respectively. While `SVC` exhibits reasonable cross-validation performance, `Nearest Neighbors` struggles to generalise well to unseen data, indicating potential overfitting or model complexity issues. These cross-validation results reinforce the findings from the earlier performance evaluation, reaffirming `Logistic Regression` as the preferred choice for predicting toxicity levels in this dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
